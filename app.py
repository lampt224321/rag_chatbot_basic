# app.py
import streamlit as st
import tempfile
import os
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from src.config import Config
from src.model_loader import ModelManager
from src.vector_db import VectorDBManager

# --- Page Config ---
st.set_page_config(page_title="Personal AI Expert RAG", layout="wide", page_icon="ü§ñ")

# --- CSS T√πy bi·∫øn ---
st.markdown("""
<style>
    .chat-message {padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex}
    .chat-message.user {background-color: #2b313e}
    .chat-message.bot {background-color: #475063}
    .source-box {font-size: 0.8em; color: #aaa; margin-top: 5px; border-top: 1px solid #555; padding-top: 5px;}
</style>
""", unsafe_allow_html=True)

st.title("ü§ñ Tr·ª£ l√Ω AI ƒê·ªçc Hi·ªÉu T√†i Li·ªáu (Deep Learning Expert Edition)")

# --- Session State Initialization ---
if "conversation" not in st.session_state:
    st.session_state.conversation = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "models_loaded" not in st.session_state:
    st.session_state.models_loaded = False

# --- Sidebar: Controls ---
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh & D·ªØ li·ªáu")
    
    # 1. Load Models (Ch·ªâ load 1 l·∫ßn)
    if not st.session_state.models_loaded:
        with st.spinner("ƒêang kh·ªüi t·∫°o AI Brain (LLM & Embeddings)..."):
            try:
                embeddings = ModelManager.load_embeddings()
                llm = ModelManager.load_llm()
                st.session_state.embeddings = embeddings
                st.session_state.llm = llm
                st.session_state.models_loaded = True
                st.success("AI ƒë√£ s·∫µn s√†ng!")
            except Exception as e:
                st.error(f"L·ªói kh·ªüi t·∫°o: {e}")
    else:
        st.success("‚úÖ AI Core Active")

    # 2. Upload File 
    uploaded_file = st.file_uploader("Upload t√†i li·ªáu PDF", type="pdf")
    
    process_btn = st.button("üöÄ X·ª≠ l√Ω t√†i li·ªáu")

# --- Main Logic: X·ª≠ l√Ω PDF ---
if process_btn and uploaded_file and st.session_state.models_loaded:
    with st.spinner("ƒêang ph√¢n t√≠ch ng·ªØ nghƒ©a (Semantic Chunking)..."):
        # L∆∞u file t·∫°m 
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            # Kh·ªüi t·∫°o VectorDB Manager
            vector_manager = VectorDBManager(st.session_state.embeddings)
            retriever = vector_manager.process_file(tmp_file_path)
            
            # D√πng ConversationalRetrievalChain thay v√¨ chain ƒë∆°n gi·∫£n
            # Gi√∫p bot nh·ªõ ƒë∆∞·ª£c ng·ªØ c·∫£nh (Memory)
            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key='answer' # Quan tr·ªçng ƒë·ªÉ chain bi·∫øt ƒë√¢u l√† output
            )

            st.session_state.conversation = ConversationalRetrievalChain.from_llm(
                llm=st.session_state.llm,
                retriever=retriever,
                memory=memory,
                return_source_documents=True, # C·∫¢I TI·∫æN: Tr·∫£ v·ªÅ ngu·ªìn
                verbose=True
            )
            
            st.success(f"ƒê√£ x·ª≠ l√Ω xong! S·∫µn s√†ng h·ªèi ƒë√°p.")
        except Exception as e:
            st.error(f"L·ªói x·ª≠ l√Ω: {e}")
        finally:
            os.unlink(tmp_file_path) # D·ªçn d·∫πp file t·∫°m

# --- Main Logic: Chat Interface [cite: 673] ---
st.subheader("üí¨ H·ªôi tho·∫°i")

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for message in st.session_state.chat_history:
    role = message["role"]
    content = message["content"]
    with st.chat_message(role):
        st.markdown(content)
        if "sources" in message and message["sources"]:
            with st.expander("üìö Ngu·ªìn tham kh·∫£o"):
                for src in message["sources"]:
                    st.markdown(f"- Trang {src['page']}: *{src['content'][:100]}...*")

# Input c√¢u h·ªèi m·ªõi
if user_question := st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu c·ªßa b·∫°n..."):
    if not st.session_state.conversation:
        st.error("Vui l√≤ng upload v√† x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc!")
    else:
        # Hi·ªÉn th·ªã c√¢u h·ªèi user
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            with st.spinner("AI ƒëang suy nghƒ©..."):
                response = st.session_state.conversation.invoke({"question": user_question})
                answer = response['answer']
                
                # Tr√≠ch xu·∫•t ngu·ªìn (Source Documents)
                source_docs = response['source_documents']
                sources_display = []
                for doc in source_docs:
                    sources_display.append({
                        "page": doc.metadata.get('page', 'N/A') + 1, # Page index starts at 0
                        "content": doc.page_content
                    })

                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi (l√†m s·∫°ch t·ª´ kh√≥a Answer: n·∫øu c√≥) 
                clean_answer = answer.split("Answer:")[-1].strip() if "Answer:" in answer else answer
                st.markdown(clean_answer)
                
                # Hi·ªÉn th·ªã ngu·ªìn
                with st.expander("üìö Ngu·ªìn tham kh·∫£o (Semantic Chunks)"):
                    for src in sources_display:
                        st.markdown(f"- **Trang {src['page']}**: {src['content'][:150]}...")
                
                # L∆∞u v√†o history
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": clean_answer,
                    "sources": sources_display
                })